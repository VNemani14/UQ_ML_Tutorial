{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678ad267",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '5' \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from progressbar import ProgressBar\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input, Reshape, Layer\n",
    "from tensorflow.keras.layers import Dropout, ReLU\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "# for SNGP\n",
    "import official.nlp.modeling.layers as nlp_layers\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "plt.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59923be",
   "metadata": {},
   "source": [
    "## Load the Capacity data\n",
    "- Three datasets: Train, Test1, Test2, Test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f20fdd22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_cells = [41,42,40,45] # train and three tests\n",
    "dataset = ['train', 'test1', 'test2', 'test3']\n",
    "\n",
    "all_Q_curves = pd.DataFrame()\n",
    "for myset in range(len(dataset)):\n",
    "    for cell in range(number_cells[myset]):\n",
    "        cell_Q = pd.read_csv(\"Dataset/discharge_capacity/\"+dataset[myset]+\"/cell\"+str(cell+1)+\".csv\", \n",
    "                               header = None, names=['cycle_no', 'capacity', 'initial_capacity'])\n",
    "        cell_Q['cell_no'] = cell+1 # cell number\n",
    "        cell_Q['norm_Q'] = cell_Q['capacity']/cell_Q['initial_capacity'].iloc[0] # cell number\n",
    "        cell_Q['dataset'] = dataset[myset]\n",
    "        all_Q_curves = pd.concat([all_Q_curves, cell_Q], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b463bf0",
   "metadata": {},
   "source": [
    "## Load the VQ curve data\n",
    "\n",
    "### We will use VQ(cycle=100)-VQ(cycle=10) as feature to determine RUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c1c1ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "VQcol_names = [\"cycle_\"+str(i+1) for i in range(150)]\n",
    "all_VQ_curves = pd.DataFrame()\n",
    "for myset in range(len(dataset)):\n",
    "    for cell in range(number_cells[myset]):\n",
    "        cell_VQ = pd.read_csv(\"Dataset/V_Q_curve/\"+dataset[myset]+\"/cell\"+str(cell+1)+\".csv\", header = None,\n",
    "                               names=VQcol_names)\n",
    "        cell_VQ['voltage'] = np.linspace(3.5,2,cell_VQ.shape[0])\n",
    "        cell_VQ['diff_cycle100_cycle10'] = cell_VQ['cycle_100']-cell_VQ['cycle_10']\n",
    "        cell_VQ['cell_no'] = cell+1 # cell number\n",
    "        cell_VQ['dataset'] = dataset[myset]\n",
    "        all_VQ_curves = pd.concat([all_VQ_curves, cell_VQ], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f00afd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_cycle_lives = pd.DataFrame()\n",
    "for myset in range(len(dataset)):\n",
    "    cycle_lives = pd.read_csv(\"Dataset/cycle_lives/\"+dataset[myset]+\"_cycle_lives.csv\", header = None,\n",
    "                             names=['cycle_life']).reset_index()\n",
    "    cycle_lives['cell_no'] = cycle_lives['index'] + 1\n",
    "    cycle_lives['dataset'] = dataset[myset]\n",
    "    \n",
    "    all_cycle_lives = pd.concat([all_cycle_lives, cycle_lives.drop('index', axis=1)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85e9472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_VQ_curves_merge = pd.merge(all_VQ_curves, all_cycle_lives, how='left',\n",
    "                               left_on = ['dataset', 'cell_no'], right_on = ['dataset', 'cell_no'])\n",
    "all_VQ_curves_merge_agg = all_VQ_curves_merge.groupby(by=['cell_no', 'dataset']).agg({'diff_cycle100_cycle10':'var',\n",
    "                                                            'cycle_life': 'mean'}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43650ee5",
   "metadata": {},
   "source": [
    "## Make Input-Output for training the UQ models\n",
    "Input:- VQ(100)-VQ(10)\n",
    "\n",
    "Output:- cycle life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e782c9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_output(df, myset):\n",
    "    df_myset = df.loc[df['dataset']==myset]\n",
    "    X, Y = [], []\n",
    "    for mycell in sorted(set(df_myset['cell_no'])):\n",
    "        X.append(df_myset.loc[df_myset['cell_no']==mycell].sort_values(\n",
    "            by='voltage', ascending = False, axis = 0)['diff_cycle100_cycle10'].values)\n",
    "        Y.append(df_myset.loc[df_myset['cell_no']==mycell]['cycle_life'].mean())\n",
    "    return X, np.expand_dims(Y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a88dece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ytrain = get_input_output(all_VQ_curves_merge, 'train')\n",
    "Xtest1, Ytest1 = get_input_output(all_VQ_curves_merge, 'test1')\n",
    "Xtest2, Ytest2 = get_input_output(all_VQ_curves_merge, 'test2')\n",
    "Xtest3, Ytest3 = get_input_output(all_VQ_curves_merge, 'test3')\n",
    "\n",
    "scalerX = MinMaxScaler()\n",
    "Xtrain_norm, Ytrain_norm  = scalerX.fit_transform(Xtrain), (Ytrain)/np.max(Ytrain)\n",
    "Xtest1_norm, Ytest1_norm  = scalerX.transform(Xtest1), (Ytest1)/np.max(Ytrain)\n",
    "Xtest2_norm, Ytest2_norm  = scalerX.transform(Xtest2), (Ytest2)/np.max(Ytrain)\n",
    "Xtest3_norm, Ytest3_norm  = scalerX.transform(Xtest3), (Ytest3)/np.max(Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6358172e",
   "metadata": {},
   "source": [
    "## Common functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ab9b943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get calibration curves\n",
    "def get_confidence_interval(y_true, mu, sigma):\n",
    "    alphas = np.linspace(1e-10, 1-1e-10, 1000)\n",
    "    myCI=[]\n",
    "    for myalpha in sorted(alphas):\n",
    "        intervals = scipy.stats.norm.interval(alpha=myalpha, loc=mu, scale=sigma)\n",
    "        lower_bd = intervals[0]\n",
    "        upper_bd = intervals[1]\n",
    "        myCI.append(np.sum((y_true > lower_bd) & (y_true < upper_bd))/len(y_true))\n",
    "    \n",
    "    return 100*alphas, 100*np.array(myCI)  # converting to percentages\n",
    "\n",
    "\n",
    "def get_rmse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    y_true - true values\n",
    "    y_pred - predicted values\n",
    "    Outputs:\n",
    "    root mean squarred error\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((y_true-y_pred)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d81620",
   "metadata": {},
   "source": [
    "## SNGP\n",
    "\n",
    "distance-aware based\n",
    "\n",
    " - spectral normalization from keras layers\n",
    " - gaussian process from gpflow, gpflux libraries\n",
    "\n",
    "(inspiration from https://secondmind-labs.github.io/GPflux/notebooks/gpflux_with_keras_layers.html and https://www.tensorflow.org/tutorials/understanding/sngp)\n",
    "\n",
    "edited source file envs/py3/lib/python3.10/site-packages/gpflux/layers/gp_layer.py\n",
    "line 212 from num_inducing ->  num_inducing.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c769db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_SNGP(tf.keras.Model):\n",
    "    def __init__(self, no_outputs, spec_norm_bound=0.9, actfn = 'tanh', **kwargs):\n",
    "        super().__init__()\n",
    "        self.actfn = actfn\n",
    "        self.spec_norm_bound = spec_norm_bound\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "        # hidden layers.\n",
    "        self.dense_layers1 = nlp_layers.SpectralNormalization(self.make_dense_layer(100),\n",
    "                                                              norm_multiplier=self.spec_norm_bound)\n",
    "        self.dense_layers2 = nlp_layers.SpectralNormalization(self.make_dense_layer(1),\n",
    "                                                              norm_multiplier=self.spec_norm_bound)\n",
    "\n",
    "        # output layer.\n",
    "        self.regressor = self.make_output_layer(no_outputs)\n",
    "\n",
    "    def call(self, inputs, training=False, return_covmat=False):\n",
    "        hidden = self.dense_layers1(inputs)\n",
    "        hidden = self.dense_layers2(hidden)\n",
    "        return self.regressor(hidden)\n",
    "\n",
    "    def make_dense_layer(self, hidden_units):\n",
    "        \"\"\"Use the Dense layer as the hidden layer.\"\"\"\n",
    "        return tf.keras.layers.Dense(hidden_units, activation=self.actfn)\n",
    "\n",
    "    def make_output_layer(self, no_outputs):\n",
    "        \"\"\"Uses Gaussian process as the output layer.\"\"\"\n",
    "        return nlp_layers.RandomFeatureGaussianProcess(\n",
    "            no_outputs,\n",
    "            gp_cov_momentum=-1,\n",
    "            **self.kwargs)\n",
    "\n",
    "class ResetCovarianceCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        \"\"\"Resets covariance matrix at the beginning of the epoch.\"\"\"\n",
    "        if epoch > 0:\n",
    "          self.model.regressor.reset_covariance_matrix()\n",
    "\n",
    "class FC_SNGPWithCovReset(FC_SNGP):\n",
    "    def fit(self, *args, **kwargs):\n",
    "        \"\"\"Adds ResetCovarianceCallback to model callbacks.\"\"\"\n",
    "        kwargs[\"callbacks\"] = list(kwargs.get(\"callbacks\", []))\n",
    "        kwargs[\"callbacks\"].append(ResetCovarianceCallback())\n",
    "        return super().fit(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6589300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trained_SNGPmodel(trainX, trainY, nepochs, actfn = 'sigmoid', spec_norm_bound = 0.9):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    trainX  - training input of shape (samples, num of features)\n",
    "    trainY  - training output of shape (samples, 1)\n",
    "    nepochs - number of epochs\n",
    "    actfn   - activation function\n",
    "    spec_norm_bound - spectral normalization bounds\n",
    "    Outputs:\n",
    "    model   - trained SNGP model\n",
    "    \"\"\"\n",
    "    loss = tf.keras.losses.MeanSquaredError()\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4)\n",
    "\n",
    "    train_config = dict(loss=loss, optimizer=optimizer)\n",
    "    resnet_config = dict(no_outputs=1, spec_norm_bound = spec_norm_bound, actfn='sigmoid')\n",
    "    fit_config = dict(batch_size=10, epochs=nepochs, verbose=0)\n",
    "\n",
    "    model = FC_SNGPWithCovReset(**resnet_config)\n",
    "    model.compile(**train_config)\n",
    "\n",
    "    model.fit(trainX, trainY, **fit_config)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b507aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SNGP = generate_trained_SNGPmodel(Xtrain_norm, Ytrain_norm, 6000, 'sigmoid', 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bbaaa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sngp_mean, sngp_covmat = model_SNGP(Xtrain_norm, return_covmat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eaa10ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE:  174.13895\n",
      "Test1 RMSE:  241.7064\n",
      "Test2 RMSE:  261.14407\n",
      "Test3 RMSE:  174.46521\n"
     ]
    }
   ],
   "source": [
    "print(\"Train RMSE: \", get_rmse(model_SNGP(Xtrain_norm, return_covmat=True)[0]*(np.max(Ytrain)), Ytrain))\n",
    "print(\"Test1 RMSE: \", get_rmse(model_SNGP(Xtest1_norm, return_covmat=True)[0]*(np.max(Ytrain)), Ytest1))\n",
    "print(\"Test2 RMSE: \", get_rmse(model_SNGP(Xtest2_norm, return_covmat=True)[0]*(np.max(Ytrain)), Ytest2))\n",
    "print(\"Test3 RMSE: \", get_rmse(model_SNGP(Xtest3_norm, return_covmat=True)[0]*(np.max(Ytrain)), Ytest3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71534f87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # parametric study for optimizing \n",
    "# EP = [5000, 10000, 20000]\n",
    "# AC = ['tanh', 'sigmoid', 'relu']\n",
    "# MU = [0.8, 0.9, 0.95]\n",
    "\n",
    "# for myact in AC:\n",
    "#     for myepoch in EP:\n",
    "#         for multiplier in MU:\n",
    "#             print(myact, myepoch, multiplier)\n",
    "#             model_SNGP = generate_trained_SNGPmodel(Xtrain_norm, Ytrain_norm, myepoch, myact, multiplier)\n",
    "#             print(\"Train RMSE: \", get_rmse(model_SNGP(Xtrain_norm, return_covmat=True)[0]*(np.max(Ytrain)), Ytrain))\n",
    "#             print(\"Test1 RMSE: \", get_rmse(model_SNGP(Xtest1_norm, return_covmat=True)[0]*(np.max(Ytrain)), Ytest1))\n",
    "#             print(\"Test2 RMSE: \", get_rmse(model_SNGP(Xtest2_norm, return_covmat=True)[0]*(np.max(Ytrain)), Ytest2))\n",
    "#             print(\"Test3 RMSE: \", get_rmse(model_SNGP(Xtest3_norm, return_covmat=True)[0]*(np.max(Ytrain)), Ytest3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8797073b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# single run\n",
    "# model_SNGP = generate_trained_SNGPmodel(Xtrain_norm, Ytrain_norm, 10000, 'relu', 0.9)\n",
    "allresults_SNGPdf = pd.DataFrame()\n",
    "for myset in dataset:\n",
    "    exec(\"Xnorm =X\"+myset+\"_norm\")\n",
    "    exec(\"Y =Y\"+myset)\n",
    "\n",
    "    result_df = pd.DataFrame(Y, columns = ['True'])\n",
    "    result_df['dataset'] = myset\n",
    "    result_df[\"SNGP_mu_eff\"] = model_SNGP(Xnorm, return_covmat=True)[0]\n",
    "    result_df[\"SNGP_sigma_eff\"] = np.sqrt(tf.linalg.diag_part(\n",
    "                                          model_SNGP(Xnorm, return_covmat=True)[1])[:, None]\n",
    "                                         )\n",
    "    allresults_SNGPdf = pd.concat([allresults_SNGPdf, result_df], axis = 0)\n",
    "\n",
    "allresults_SNGPdf[\"SNGP_mu_eff\"] = allresults_SNGPdf[\"SNGP_mu_eff\"]*(np.max(Ytrain))\n",
    "allresults_SNGPdf[\"SNGP_sigma_eff\"] = allresults_SNGPdf[\"SNGP_sigma_eff\"]*(np.max(Ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3e938e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |#########################################################################|\n"
     ]
    }
   ],
   "source": [
    "model_train_bool = True\n",
    "if model_train_bool:\n",
    "    nmodels = 3 # total models trained and best one picked\n",
    "    niter = 10\n",
    "\n",
    "    allCI_SNGP = np.zeros((1000, niter))\n",
    "    pbar=ProgressBar()\n",
    "    allresults_SNGPdf = pd.DataFrame()\n",
    "\n",
    "    for myiter in pbar(range(niter)): # for each independent iteration\n",
    "        all_models_SNGP=[]\n",
    "        all_rmse_train_SNGP = np.zeros(nmodels,)\n",
    "        for i in range(nmodels): # train multiple models to select the best\n",
    "            model_SNGP = generate_trained_SNGPmodel(Xtrain_norm, Ytrain_norm, 10000, 'relu', 0.9)\n",
    "            all_rmse_train_SNGP[i] = get_rmse(model_SNGP(Xtrain_norm, return_covmat=True)[0]*(np.max(Ytrain)), \n",
    "                                             Ytrain)\n",
    "            all_models_SNGP.append(model_SNGP)\n",
    "        select_modelSNGP = all_models_SNGP[np.argmin(all_rmse_train_SNGP)] # best model\n",
    "\n",
    "             \n",
    "        for myset in dataset:\n",
    "            exec(\"Xnorm =X\"+myset+\"_norm\")\n",
    "            exec(\"Y =Y\"+myset)\n",
    "\n",
    "            result_df = pd.DataFrame(Y, columns = ['True'])\n",
    "            result_df['dataset'] = myset\n",
    "            result_df[\"SNGP_mu_eff\"] = select_modelSNGP(Xnorm, return_covmat=True)[0]\n",
    "            result_df[\"SNGP_sigma_eff\"] = np.sqrt(tf.linalg.diag_part(\n",
    "                                          select_modelSNGP(Xnorm, return_covmat=True)[1])[:, None]\n",
    "                                                 )\n",
    "            result_df['iteration'] = myiter\n",
    "            allresults_SNGPdf = pd.concat([allresults_SNGPdf, result_df], axis = 0)\n",
    "               \n",
    "    allresults_SNGPdf[\"SNGP_mu_eff\"] = allresults_SNGPdf[\"SNGP_mu_eff\"]*(np.max(Ytrain))\n",
    "    allresults_SNGPdf[\"SNGP_sigma_eff\"] = allresults_SNGPdf[\"SNGP_sigma_eff\"]*(np.max(Ytrain))\n",
    "    allresults_SNGPdf.to_excel(\"SNGP_battery_prediction_results.xlsx\")  \n",
    "else:\n",
    "    allresults_SNGPdf=pd.read_excel(\"SNGP_battery_prediction_results.xlsx\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
