{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9483b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 11:39:27.975252: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-19 11:39:28.932743: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 11:39:28.932826: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 11:39:28.932834: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import seaborn as sns\n",
    "from progressbar import ProgressBar\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Input, Conv1D, MaxPooling1D, Reshape, Flatten, GlobalMaxPooling1D, Layer\n",
    "from tensorflow.keras.layers import Dropout, ReLU\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from keras.initializers import glorot_normal\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import mean_squared_error as rmse\n",
    "\n",
    "plt.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0217982",
   "metadata": {},
   "source": [
    "## Load the new CMAPPS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "350bc460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unit11_win1_str1_smp10.npz', 'Unit2_win1_str1_smp10.npz', 'Unit10_win1_str1_smp10.npz', 'Unit14_win1_str1_smp10.npz', 'Unit5_win1_str1_smp10.npz', 'Unit18_win1_str1_smp10.npz', 'Unit16_win1_str1_smp10.npz', 'Unit15_win1_str1_smp10.npz', 'Unit20_win1_str1_smp10.npz']\n"
     ]
    }
   ],
   "source": [
    "mypath = 'N-CMAPSS_DL/N-CMAPSS/Samples_whole/'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "print(onlyfiles)\n",
    "\n",
    "train_units = [2,5,10,16,18,20]\n",
    "test_units = [11,14,15]\n",
    "\n",
    "test_files = [[f for f in onlyfiles if str(test_units[j]) in f ] for j in range(len(test_units))]\n",
    "test_files = np.array(test_files)[:,0].tolist()\n",
    "train_files = list(set(onlyfiles) - set(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5be33b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_part_array_merge (current_dir, npz_units):\n",
    "    sample_array_lst = []\n",
    "    label_array_lst = []\n",
    "    for npz_unit in npz_units:\n",
    "      loaded = np.load(current_dir + npz_unit)\n",
    "      sample_array_lst.append(loaded['sample'])\n",
    "      label_array_lst.append(loaded['label'])\n",
    "    sample_array = np.dstack(sample_array_lst)\n",
    "    label_array = np.concatenate(label_array_lst)\n",
    "    sample_array = sample_array.transpose(2, 0, 1)\n",
    "    return sample_array, label_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b6a672",
   "metadata": {},
   "source": [
    "## Make Input-Output for training the UQ models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5609490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_norm,Ytrain = load_part_array_merge(mypath,train_files)\n",
    "Xtest_norm,Ytest = load_part_array_merge(mypath,test_files)\n",
    "Xtrain_norm = Xtrain_norm[:,0,:]\n",
    "Xtest_norm = Xtest_norm[:,0,:]\n",
    "Ytrain_norm = (Ytrain)/np.max(Ytrain)\n",
    "Ytest_norm = (Ytest)/np.max(Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15918ace",
   "metadata": {},
   "source": [
    "## Uncertainty Model 1: Deep Ensemble (DE)\n",
    "\n",
    "- Use of Gaussian Layer that outputs - mean and aleatoric variance\n",
    "- Use of Negative Log Likelihood loss\n",
    "- Ensemble to capture epistematic uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f7a1e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function and new custom layer to give mean and standard deviation\n",
    "def custom_loss(sigma):\n",
    "    def gaussian_loss(y_true, y_pred):\n",
    "        return tf.reduce_mean(0.5*tf.math.log((sigma)) + 0.5*tf.math.divide(tf.math.square(y_true - y_pred), (sigma))) + 1e-6\n",
    "    return gaussian_loss\n",
    "class GaussianLayer(Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(GaussianLayer, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.kernel_1 = self.add_weight(name='kernel_1', \n",
    "                                      shape=(10, self.output_dim),\n",
    "                                      initializer=glorot_normal(),\n",
    "                                      trainable=True)\n",
    "        self.kernel_2 = self.add_weight(name='kernel_2', \n",
    "                                      shape=(10, self.output_dim),\n",
    "                                      initializer=glorot_normal(),\n",
    "                                      trainable=True)\n",
    "        self.bias_1 = self.add_weight(name='bias_1',\n",
    "                                    shape=(self.output_dim, ),\n",
    "                                    initializer=glorot_normal(),\n",
    "                                    trainable=True)\n",
    "        self.bias_2 = self.add_weight(name='bias_2',\n",
    "                                    shape=(self.output_dim, ),\n",
    "                                    initializer=glorot_normal(),\n",
    "                                    trainable=True)\n",
    "        super(GaussianLayer, self).build(input_shape) \n",
    "    def call(self, x):\n",
    "        output_mu  = K.dot(x, self.kernel_1) + self.bias_1\n",
    "        output_sig = K.dot(x, self.kernel_2) + self.bias_2\n",
    "        output_sig_pos = K.log(1 + K.exp(output_sig)) + 1e-06  \n",
    "        return [output_mu, output_sig_pos]\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [(input_shape[0], self.output_dim), (input_shape[0], self.output_dim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c06af574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trained_DEmodel(trainX, trainY, nepochs, actfn = 'sigmoid'):\n",
    "    n_inp_features = np.shape(trainX)[1]\n",
    "    feature_input = Input(shape=(n_inp_features,), name=\"feature_input_layer\")\n",
    "    x = Dense(100, activation = actfn)(feature_input)\n",
    "    x = Dense(10, activation = actfn)(x)\n",
    "      \n",
    "    mu, sigma = GaussianLayer(1, name='main_output')(x)\n",
    "    model = Model(feature_input, mu)\n",
    "    model.compile(loss=custom_loss(sigma), optimizer='adam')\n",
    "    model.fit(trainX, trainY,shuffle=True, epochs=nepochs, verbose = 1)\n",
    "    \n",
    "    layer_name = 'main_output' # Where to extract the Gaussian output from. \n",
    "    get_intermediate = K.function(inputs=[model.input], outputs=model.get_layer(layer_name).output)\n",
    "    return get_intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5361722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% |                                                                                                                                                                    |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 526345 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 11:39:58.237903: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-19 11:39:58.238609: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-19 11:39:58.238630: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-19 11:39:58.239643: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-19 11:39:58.266915: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2022-12-19 11:39:58.304422: W tensorflow/c/c_api.cc:291] Operation '{name:'training/Adam/dense_1/bias/v/Assign' id:380 op device:{requested: '', assigned: ''} def:{{{node training/Adam/dense_1/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/dense_1/bias/v, training/Adam/dense_1/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526345/526345 [==============================] - 24s 46us/sample - loss: -1.7489\n",
      "Epoch 2/20\n",
      "526345/526345 [==============================] - 24s 45us/sample - loss: -2.1300\n",
      "Epoch 3/20\n",
      "526345/526345 [==============================] - 24s 45us/sample - loss: -2.2111\n",
      "Epoch 4/20\n",
      "526345/526345 [==============================] - 24s 45us/sample - loss: -2.2580\n",
      "Epoch 5/20\n",
      "526345/526345 [==============================] - 24s 45us/sample - loss: -2.2871\n",
      "Epoch 6/20\n",
      "526345/526345 [==============================] - 24s 45us/sample - loss: -2.3079\n",
      "Epoch 7/20\n",
      "526345/526345 [==============================] - 24s 45us/sample - loss: -2.3311\n",
      "Epoch 8/20\n",
      "526345/526345 [==============================] - 24s 45us/sample - loss: -2.3548\n",
      "Epoch 9/20\n",
      "526345/526345 [==============================] - 24s 46us/sample - loss: -2.3695\n",
      "Epoch 10/20\n",
      "526345/526345 [==============================] - 24s 45us/sample - loss: -2.3792\n",
      "Epoch 11/20\n",
      "526345/526345 [==============================] - 24s 46us/sample - loss: -2.3888\n",
      "Epoch 12/20\n",
      "526345/526345 [==============================] - 24s 45us/sample - loss: -2.3926\n",
      "Epoch 13/20\n",
      "526345/526345 [==============================] - 24s 46us/sample - loss: -2.4026\n",
      "Epoch 14/20\n",
      "526345/526345 [==============================] - 24s 46us/sample - loss: -2.4061\n",
      "Epoch 15/20\n",
      "526345/526345 [==============================] - 24s 46us/sample - loss: -2.4150\n",
      "Epoch 16/20\n",
      "526345/526345 [==============================] - 24s 45us/sample - loss: -2.4237\n",
      "Epoch 17/20\n",
      "526345/526345 [==============================] - 24s 45us/sample - loss: -2.4298\n",
      "Epoch 18/20\n",
      "526345/526345 [==============================] - 24s 45us/sample - loss: -2.4304\n",
      "Epoch 19/20\n",
      "395680/526345 [=====================>........] - ETA: 5s - loss: -2.4322"
     ]
    }
   ],
   "source": [
    "## Create 20 models and select some based on training error\n",
    "prediction_fns, train_mae = [], []\n",
    "nmodels = 2\n",
    "nepochs = 20\n",
    "\n",
    "pbar=ProgressBar()\n",
    "for i in pbar(range(nmodels)):\n",
    "    my_func = generate_trained_DEmodel(Xtrain_norm, Ytrain_norm, nepochs, actfn = 'tanh')\n",
    "    train_mae.append(mae(Ytrain, (my_func(Xtrain_norm)[0])*np.max(Ytrain)))\n",
    "    prediction_fns.append(my_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8db0ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmodels_select = 2\n",
    "final_DE_fns = []\n",
    "for i in range(nmodels_select):\n",
    "    final_DE_fns.append(prediction_fns[np.argsort(train_mae)[:nmodels_select][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c877db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble(df, UQmodelcode, nmodels_select):\n",
    "    mu_cols = [UQmodelcode + \"_mu_\"+str(i) for i in range(nmodels_select)]\n",
    "    sigma_cols = [UQmodelcode + \"_sigma_\"+str(i) for i in range(nmodels_select)]\n",
    "    \n",
    "    mu_eff = df[mu_cols].mean(axis=1)\n",
    "    if UQmodelcode==\"DE\":\n",
    "        sigma_eff =  np.sqrt(np.mean(df[mu_cols]**2, axis = 1)\n",
    "                             + np.mean(df[sigma_cols]**2, axis = 1)\n",
    "                             - mu_eff**2)\n",
    "    elif UQmodelcode==\"MC\":\n",
    "        sigma_eff =  np.sqrt(np.mean(df[mu_cols]**2, axis = 1)\n",
    "                             - mu_eff**2)\n",
    "    df[UQmodelcode+\"_mu_eff\"] = mu_eff\n",
    "    df[UQmodelcode+\"_sigma_eff\"] = sigma_eff\n",
    "    return df\n",
    "\n",
    "allresults_df = pd.DataFrame()\n",
    "dataset = ['train', 'test']\n",
    "for myset in dataset:\n",
    "    exec(\"Xnorm =X\"+myset+\"_norm\")\n",
    "    exec(\"Y =Y\"+myset)\n",
    "\n",
    "    result_df = pd.DataFrame(Y, columns = ['True'])\n",
    "    result_df['dataset'] = myset\n",
    "    for i in range(nmodels_select):\n",
    "        result_df[\"DE_mu_\"+str(i)] = (final_DE_fns[i](Xnorm)[0])*np.max(Ytrain)\n",
    "        result_df[\"DE_sigma_\"+str(i)] = (np.sqrt(final_DE_fns[i](Xnorm)[1]))*np.max(Ytrain)\n",
    "\n",
    "    result_df = get_ensemble(result_df, \"DE\", nmodels_select)\n",
    "    allresults_df = pd.concat([allresults_df, result_df], axis = 0)\n",
    "allresults_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe0a87c",
   "metadata": {},
   "source": [
    "## Results - Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccd9056",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = allresults_df[allresults_df['dataset']=='test']['True'].values\n",
    "y_pred = allresults_df[allresults_df['dataset']=='test']['DE_mu_0'].values\n",
    "sigma_pred = allresults_df[allresults_df['dataset']=='test']['DE_sigma_0'].values\n",
    "plt.figure()\n",
    "plt.plot(y_pred)\n",
    "plt.plot(y_true)\n",
    "plt.figure()\n",
    "plt.plot(sigma_pred)\n",
    "plt.ylim(0,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_interval(y_true, mu, sigma):\n",
    "    alphas = np.linspace(1e-10, 1-1e-10, 1000)\n",
    "    myCI=[]\n",
    "    for myalpha in tqdm(sorted(alphas)):\n",
    "        intervals = scipy.stats.norm.interval(alpha=myalpha, loc=mu, scale=sigma)\n",
    "        lower_bd = intervals[0]\n",
    "        upper_bd = intervals[1]\n",
    "        myCI.append(np.sum((y_true > lower_bd) & (y_true < upper_bd))/len(y_true))\n",
    "    \n",
    "    return 100*alphas, 100*np.array(myCI)  # converting to percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aafb13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas, CI_DE = get_confidence_interval(allresults_df_sorted['True'].values, \n",
    "                                       allresults_df_sorted['DE_mu_eff'].values, \n",
    "                                       allresults_df_sorted['DE_sigma_eff'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b8a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.plot(alphas,CI_DE, color = 'blue', label='Deep Ensemble')\n",
    "plt.plot([0,100],[0,100], color='black', linestyle='dashed', label='Ideal')\n",
    "plt.xlabel('Expected Confidence')\n",
    "plt.ylabel('Precited Confidence')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347d1851",
   "metadata": {},
   "source": [
    "## Repeat the above algorithm for 10 times to capture variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6baf9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multiple runs\n",
    "niter = 10\n",
    "allCI_DE = np.zeros((1000, niter))\n",
    "pbar=ProgressBar()\n",
    "\n",
    "for myiter in pbar(range(niter)):\n",
    "    prediction_fns=[]\n",
    "    train_mae=[]\n",
    "    for i in range(nmodels):\n",
    "        my_func = generate_trained_DEmodel(Xtrain_norm, Ytrain_norm, nepochs, actfn = 'tanh')\n",
    "        train_mae.append(mae(Ytrain, (my_func(Xtrain_norm)[0])*np.max(Ytrain)))\n",
    "        prediction_fns.append(my_func)\n",
    "\n",
    "    nmodels_select = 2\n",
    "    final_DE_fns = []\n",
    "    for i in range(nmodels_select):\n",
    "        final_DE_fns.append(prediction_fns[np.argsort(train_mae)[:nmodels_select][i]])\n",
    "\n",
    "    allresults_df = pd.DataFrame()\n",
    "    for myset in dataset:\n",
    "        exec(\"Xnorm =X\"+myset+\"_norm\")\n",
    "        exec(\"Y =Y\"+myset)\n",
    "\n",
    "        result_df = pd.DataFrame(Y, columns = ['True'])\n",
    "        result_df['dataset'] = myset\n",
    "        for i in range(nmodels_select):\n",
    "            result_df[\"DE_mu_\"+str(i)] = (final_DE_fns[i](Xnorm)[0])*np.max(Ytrain)\n",
    "            result_df[\"DE_sigma_\"+str(i)] = (np.sqrt(final_DE_fns[i](Xnorm)[1]))*np.max(Ytrain)\n",
    "\n",
    "        result_df = get_ensemble(result_df, \"DE\", nmodels_select)\n",
    "        allresults_df = pd.concat([allresults_df, result_df], axis = 0)\n",
    "    allresults_df.head()\n",
    "\n",
    "    allresults_df_sorted = allresults_df.sort_values(by='True', ascending=True)\n",
    "\n",
    "    alphas, CI_DE = get_confidence_interval(allresults_df_sorted['True'].values, \n",
    "                                           allresults_df_sorted['DE_mu_eff'].values, \n",
    "                                           allresults_df_sorted['DE_sigma_eff'].values)\n",
    "    allCI_DE[:,myiter] = CI_DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd558c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.plot([0,100],[0,100], color='black', linestyle='dashed', label='Ideal')\n",
    "plt.fill_between(alphas,\n",
    "                np.mean(allCI_DE, axis = 1)+np.std(allCI_DE, axis = 1),\n",
    "                np.mean(allCI_DE, axis = 1)-np.std(allCI_DE, axis = 1),\n",
    "                color='blue', alpha = 0.4, linestyle='None', label='Deep Ensemble')\n",
    "plt.xlabel('Expected Confidence')\n",
    "plt.ylabel('Precited Confidence')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1937bf8e",
   "metadata": {},
   "source": [
    "## UQ Model: MC Dropout\n",
    "\n",
    "Simple MLP architecture with 10% dropout probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76381e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trained_MCmodel(trainX, trainY, nepochs, actfn = 'sigmoid'):\n",
    "\n",
    "    n_inp_features = np.shape(trainX)[1]\n",
    "    feature_input = Input(shape=(n_inp_features,), name=\"feature_input_layer\")\n",
    "    x = Dense(100, activation = actfn)(feature_input)\n",
    "    x = Dropout(rate = 0.10)(x)\n",
    "    x = Dense(10, activation = actfn)(x)\n",
    "    x = Dropout(rate = 0.10)(x)\n",
    "    mu = Dense(1, activation = actfn)(x)\n",
    "\n",
    "    model = Model(feature_input, mu)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(trainX, trainY,shuffle=True, epochs=nepochs, verbose = 1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9e00f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmodels = 2\n",
    "all_models_MC=[]\n",
    "all_mae_train_MC = np.zeros(nmodels,)\n",
    "pbar = ProgressBar()\n",
    "for i in pbar(range(nmodels)):\n",
    "    model_MC = generate_trained_MCmodel(Xtrain_norm, Ytrain_norm, 20, 'sigmoid')\n",
    "    all_mae_train_MC[i] = mae(K.get_value(model_MC(Xtrain_norm))*(np.max(Ytrain)), Ytrain)\n",
    "    all_models_MC.append(model_MC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a680a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_modelMC = all_models_MC[np.argmin(all_mae_train_MC)]\n",
    "print(\"Train MAE: \", mae(K.get_value(select_modelMC(Xtrain_norm))*(np.max(Ytrain)), Ytrain))\n",
    "print(\"Test1 MAE: \", mae(K.get_value(select_modelMC(Xtest_norm))*(np.max(Ytrain)), Ytest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025b3778",
   "metadata": {},
   "outputs": [],
   "source": [
    "allresults_MCdf = pd.DataFrame()\n",
    "for myset in dataset:\n",
    "    exec(\"Xnorm =X\"+myset+\"_norm\")\n",
    "    exec(\"Y =Y\"+myset)\n",
    "\n",
    "    result_df = pd.DataFrame(Y, columns = ['True'])\n",
    "    result_df['dataset'] = myset\n",
    "    for i in range(nmodels_select):\n",
    "        result_df[\"MC_mu_\"+str(i)] = K.get_value(select_modelMC(Xnorm, training=True))*(np.max(Ytrain))\n",
    "\n",
    "    allresults_MCdf = pd.concat([allresults_MCdf, result_df], axis = 0)\n",
    "allresults_MCdf = get_ensemble(allresults_MCdf, \"MC\", nmodels_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37bd476",
   "metadata": {},
   "source": [
    "## Results - Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c67d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas, CI_DE = get_confidence_interval(allresults_MCdf['True'].values, \n",
    "                                       allresults_MCdf['MC_mu_eff'].values, \n",
    "                                       allresults_MCdf['MC_sigma_eff'].values)\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.plot(alphas,CI_DE, color = 'blue', label='MC Dropout')\n",
    "plt.plot([0,100],[0,100], color='black', linestyle='dashed', label='Ideal')\n",
    "plt.xlabel('Expected Confidence')\n",
    "plt.ylabel('Precited Confidence')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f3de50",
   "metadata": {},
   "source": [
    "## Repeat the above algorithm for 10 times to capture variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6bd38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repeat 10 times\n",
    "nmodels = 5 # total models trained and best one picked\n",
    "nmodels_select = 5 # run 5 times per each select model with dropout\n",
    "niter = 10\n",
    "\n",
    "allCI_MC = np.zeros((1000, niter))\n",
    "pbar=ProgressBar()\n",
    "\n",
    "for myiter in pbar(range(niter)):\n",
    "    all_models_MC=[]\n",
    "    all_mae_train_MC = np.zeros(nmodels,)\n",
    "    pbar = ProgressBar()\n",
    "    for i in range(nmodels):\n",
    "        model_MC = generate_trained_MCmodel(Xtrain_norm, Ytrain_norm, 3000, 'sigmoid')\n",
    "        all_mae_train_MC[i] = mae(K.get_value(model_MC(Xtrain_norm))*(np.max(Ytrain)), Ytrain)\n",
    "        all_models_MC.append(model_MC)\n",
    "\n",
    "    select_modelMC = all_models_MC[np.argmin(all_mae_train_MC)]\n",
    "\n",
    "    allresults_MCdf = pd.DataFrame()\n",
    "    for myset in dataset:\n",
    "        exec(\"Xnorm =X\"+myset+\"_norm\")\n",
    "        exec(\"Y =Y\"+myset)\n",
    "\n",
    "        result_df = pd.DataFrame(Y, columns = ['True'])\n",
    "        result_df['dataset'] = myset\n",
    "        for i in range(nmodels_select):\n",
    "            result_df[\"MC_mu_\"+str(i)] = K.get_value(select_modelMC(Xnorm, training=True))*(np.max(Ytrain))\n",
    "\n",
    "        allresults_MCdf = pd.concat([allresults_MCdf, result_df], axis = 0)\n",
    "    allresults_MCdf = get_ensemble(allresults_MCdf, \"MC\", nmodels_select)\n",
    "\n",
    "    alphas, CI_MC = get_confidence_interval(allresults_MCdf['True'].values, \n",
    "                                           allresults_MCdf['MC_mu_eff'].values, \n",
    "                                           allresults_MCdf['MC_sigma_eff'].values)\n",
    "    allCI_MC[:,myiter] = CI_MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507ccf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.plot([0,100],[0,100], color='black', linestyle='dashed', label='Ideal')\n",
    "plt.fill_between(alphas,\n",
    "                np.mean(allCI_MC, axis = 1)+np.std(allCI_MC, axis = 1),\n",
    "                np.mean(allCI_MC, axis = 1)-np.std(allCI_MC, axis = 1),\n",
    "                color='red', alpha = 0.4, linestyle='None', label='MC-Dropout')\n",
    "plt.fill_between(alphas,\n",
    "                np.mean(allCI_DE, axis = 1)+np.std(allCI_DE, axis = 1),\n",
    "                np.mean(allCI_DE, axis = 1)-np.std(allCI_DE, axis = 1),\n",
    "                color='blue', alpha = 0.4, linestyle='None', label='Deep Ensemble')\n",
    "\n",
    "plt.xlabel('Expected Confidence')\n",
    "plt.ylabel('Observed Confidence')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f7d0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UQ",
   "language": "python",
   "name": "uq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
